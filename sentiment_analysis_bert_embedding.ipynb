{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA_bert_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2hBCyP37DfTJ",
        "ea-nMBIMAV7_",
        "0JtgEviBAcew",
        "FPIWIoBeDlwn",
        "EdNs9Mns7ERL",
        "FIpeakPP7Gci",
        "50fQSLEl72OE",
        "f_TSVOsv-K55",
        "CvWEiZxw-QGn",
        "o1-yytKIDe75",
        "kTgdxwC7I26L",
        "dUrIOeUsIoQw",
        "mHVo1_-FO6hr",
        "7AcQbsXERDTd",
        "f0uirgCUTVy6",
        "qM5hWUTnEJ6H",
        "agNUC9xToAcR",
        "PWnNKVDboIpa",
        "CCFdXyabpAkw",
        "olKb-XUHpyri",
        "LP_BOLP0qgwZ",
        "6jSyyYBxEJP3"
      ],
      "authorship_tag": "ABX9TyOvp5P7TZz9mRN6ChD09gaV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArazShilabin/Agglomerative-Hierarchical-Clustering-Text-Data/blob/master/sentiment_analysis_bert_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hBCyP37DfTJ"
      },
      "source": [
        "### mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at00gH74_82C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "930faa69-e96b-4360-f0a6-96a052b74dec"
      },
      "source": [
        "\"\"\" \r\n",
        "Use this javascript code in inspect>console so you wont need to click the page every 15 min:\r\n",
        "\r\n",
        "########################\r\n",
        "function ConnectButton(){\r\n",
        "    console.log(\"Connect pushed\"); \r\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \r\n",
        "}\r\n",
        "setInterval(ConnectButton,60000);\r\n",
        "########################\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%pwd\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea-nMBIMAV7_"
      },
      "source": [
        "### change current path to where the working project folder is at"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax6n6V_MAOYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "90c9fa22-a32c-45a1-f468-6b0453a7566a"
      },
      "source": [
        "%cd drive/MyDrive/projects/bert_tokenizer_SA/\r\n",
        "%pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/projects/bert_tokenizer_SA\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/projects/bert_tokenizer_SA'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f88TsAbxEAv6"
      },
      "source": [
        "# Step 0: Get The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JtgEviBAcew"
      },
      "source": [
        "### upload the data to our current path and unzip it (uncomment and run this only once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEQhpE2SAdM_"
      },
      "source": [
        "# # data is from: http://help.sentiment140.com/for-students you can use this or just upload your own data\r\n",
        "# %cd data\r\n",
        "# !wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip # download\r\n",
        "# !unzip trainingandtestdata.zip # unzip the downloaded file\r\n",
        "# %cd .. \r\n",
        "# %pwd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPIWIoBeDlwn"
      },
      "source": [
        "# Step 1: Importing Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdNs9Mns7ERL"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DMMjWcHCtTx"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import math\r\n",
        "import time\r\n",
        "from bs4 import BeautifulSoup # we use this library to turn the tweets to texts\r\n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIpeakPP7Gci"
      },
      "source": [
        "### Install Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwSlc2H67aux",
        "outputId": "20bf87df-b7a4-478a-a370-3ab407514e62"
      },
      "source": [
        "# this is not the official one, it's a lighter one\r\n",
        "!pip install bert-for-tf2\r\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d3/820ccaf55f1e24b5dd43583ac0da6d86c2d27bbdfffadbba69bafe73ca93/bert-for-tf2-0.14.7.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 21.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.7-cp36-none-any.whl size=30537 sha256=ec4593ecf945bfe1f7a04618d51dde30030f6636c98ffbe240e0fed4c742cfff\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/f8/e2/b98f79a6b8cc898d8e4102b83acb8a098df7d27500a2bac912\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7303 sha256=9fffcb2439c4199936a8f61a33d866b36bb1b3583e3e882281dd283709e35a44\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19474 sha256=4b64c57ed5c0e3a15f87b6f94a6218e982f4a326f717751c2ee51b09eccb9351\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.7 params-flow-0.8.2 py-params-0.9.7\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 22.2MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50fQSLEl72OE"
      },
      "source": [
        "### Add tensorflow packages and Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA5szEBu77M6"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "import tensorflow_hub as hub # all the pretrained models are installed in tensorflow hub\r\n",
        "import bert"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJV12RZ4EKL_"
      },
      "source": [
        "# Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_TSVOsv-K55"
      },
      "source": [
        "## A) Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXDoO-XVEvf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "29b51b74-bed5-4f16-abc1-296b72f74335"
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\r\n",
        "data = pd.read_csv(\r\n",
        "    \"data/training.1600000.processed.noemoticon.csv\",\r\n",
        "    header=None,\r\n",
        "    names=cols,\r\n",
        "    engine=\"python\", # we need this for this specific data\r\n",
        "    encoding=\"latin1\") # the encoded format\r\n",
        "\r\n",
        "data.drop([\"id\",\"date\",\"query\",\"user\"], axis=1, inplace=True) # drop the useless cols\r\n",
        "data.head(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvWEiZxw-QGn"
      },
      "source": [
        "## B) Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQw8ePJ-U6C"
      },
      "source": [
        "def clean_tweet(tweet):\r\n",
        "    # the lmxl is the encoding it has, so beautifulsoup turns it to readable txt\r\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\r\n",
        "    # remove the @'s (*: appear 0 or more)\r\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]*\", ' ', tweet)\r\n",
        "    # remove \"http://\" or \"https://\" (this s? means that 's' can be or not)\r\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9]*\", ' ', tweet)\r\n",
        "    # get rid of everything that is not a letter or \".!?'\"\r\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\",' ', tweet)\r\n",
        "    # replace >=2 white spaces with one (+: appear at least once)\r\n",
        "    tweet = re.sub(\" +\", ' ', tweet)\r\n",
        "    return tweet"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syKavfUiArJu"
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data['text']]\r\n",
        "data_labels = data['sentiment'].values\r\n",
        "data_labels[data_labels == 4] = 1 # for some reason the labels instead of 0's and 1's, they are 0's and 4's so we turn the 4's to 1's"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1-yytKIDe75"
      },
      "source": [
        "## C) Tokenization (using Bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTgdxwC7I26L"
      },
      "source": [
        "### Load tokenizer (add 'clrs' & 'sep' too)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbkHGQBSA8Ho"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\r\n",
        "# all the pretrained models are installed in tensorflow hub, we can find many \r\n",
        "# diffenet kinds in Hub, we can choose differet versions from hub, now we \r\n",
        "# got the weights of the layer (these are the SavedModels)\r\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\r\n",
        "                            trainable=False)\r\n",
        "\r\n",
        "# get the vocabulary from the layer\r\n",
        "vocab_file =  bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() # do lowercase\r\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case) # get the tokenizer\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-9Jt1heJ6xi"
      },
      "source": [
        "def encode_sentences(sentence):\r\n",
        "    return [\"[CLS]\"] + tokenizer.tokenize(sentence) + [\"[SEP]\"]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL4ZZwCFKL7R"
      },
      "source": [
        "data_inputs = [encode_sentences(sentence) for sentence in data_clean] # encode inputs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUrIOeUsIoQw"
      },
      "source": [
        "### Test our tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCXY_Ny0InIJ",
        "outputId": "e0048295-c61e-45c1-f79c-4f95649a62cc"
      },
      "source": [
        "test_text = \"Tensorflow is great, ain't it?\"\r\n",
        "print(tokenizer.tokenize(test_text))\r\n",
        "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_text)))\r\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tensor', '##flow', 'is', 'great', ',', 'ain', \"'\", 't', 'it', '?']\n",
            "[23435, 12314, 2003, 2307, 1010, 7110, 1005, 1056, 2009, 1029]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1piGn2MrK5qp"
      },
      "source": [
        "## D) Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q1gy2PR7tGF"
      },
      "source": [
        "### we need to create 3 different inputs for each sentence (ids, mask, sentence_segments)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIKvD2Br7shX"
      },
      "source": [
        "def get_ids(tokens):\r\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "def get_mask(tokens): # gets the mask: if token is \"[pad]\" then 0. else 1.\r\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\r\n",
        "\r\n",
        "def get_segments(tokens):\r\n",
        "    seg_ids = []\r\n",
        "    current_seg_id = 0\r\n",
        "    for tok in tokens:\r\n",
        "        seg_ids.append(current_seg_id)\r\n",
        "        if tok == \"[SEP]\":\r\n",
        "            # 0->1 and 1->0, why? we are segmenting the 2 sentences 000...01...111\r\n",
        "            current_seg_id ^= 1 \r\n",
        "    return seg_ids"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRxG2tjQew6e"
      },
      "source": [
        "### shuffle then sort them so we can choose simmilar size batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95TWyDtfbPUI"
      },
      "source": [
        "# we get [text, label, len_text] we want this for padding purposes later\r\n",
        "data_with_len = [[sent, data_labels[i], len(sent)]\r\n",
        "                 for i, sent in enumerate(data_inputs)]\r\n",
        "random.shuffle(data_with_len)\r\n",
        "data_with_len.sort(key=lambda x: x[2]) # sort on len\r\n",
        "\r\n",
        "sorted_all = [([get_ids(sent_lab[0]),\r\n",
        "                get_mask(sent_lab[0]),\r\n",
        "                get_segments(sent_lab[0])],\r\n",
        "               sent_lab[1])\r\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 3] # short sentences probably have no sentiment"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHVo1_-FO6hr"
      },
      "source": [
        "### Dataset generator (because our data isn't the same size, needs padding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU-av4nCL3d6",
        "outputId": "9926559e-671d-48d0-950f-edf0463f5905"
      },
      "source": [
        "all_dataset = tf.data.Dataset.from_generator(generator=lambda: sorted_all, # generator needs to be callable\r\n",
        "                                             output_types=(tf.int32,tf.int32)) # (inputs, labels) both are int\r\n",
        "print(type(all_dataset))\r\n",
        "print(next(iter(all_dataset)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.FlatMapDataset'>\n",
            "(<tf.Tensor: shape=(3, 4), dtype=int32, numpy=\n",
            "array([[ 101, 2021, 4283,  102],\n",
            "       [   1,    1,    1,    1],\n",
            "       [   0,    0,    0,    0]], dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AcQbsXERDTd"
      },
      "source": [
        "### Batching & Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zO4YaB-RFu2",
        "outputId": "b72aa359-5411-41b1-893d-860752c484b4"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "# turn it to padded\r\n",
        "all_batched = all_dataset.padded_batch(\r\n",
        "                batch_size=BATCH_SIZE,\r\n",
        "                padded_shapes=((3,None),())\r\n",
        "                # None here means pad as much as you need (it depends on the len of our sentence)\r\n",
        ")\r\n",
        "print(type(all_batched))\r\n",
        "print(next(iter(all_batched)))\r\n",
        "# here they are all size 4 because it's the first batch, but later there\r\n",
        "# will be a mixture of size for example 3&4 so the 3's will be padded to 4"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.PaddedBatchDataset'>\n",
            "(<tf.Tensor: shape=(32, 3, 4), dtype=int32, numpy=\n",
            "array([[[  101,  2021,  4283,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2149,  2243,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2428,  1029,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4957,  1029,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  7989, 13825,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2023, 19237,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101, 15624,  3599,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4067,  2017,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  1045,  2113,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2009, 13403,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  3015,  3343,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  6160,  2067,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2178,  2154,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4067, 29337,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2665, 13435,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2748,  2126,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4067,  1057,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101, 23156,   999,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4283,   999,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4283,  4901,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4067,  2017,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4283,  2017,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  3376,  2154,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  3067,   999,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2205,  2919,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4283,  2158,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  1045,  5993,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  4067,  2017,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2197,  2154,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2115,  6160,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  3374,  3336,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]],\n",
            "\n",
            "       [[  101,  2572,  3147,   102],\n",
            "        [    1,     1,     1,     1],\n",
            "        [    0,     0,     0,     0]]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
            "array([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 0, 1, 1, 1, 1, 1, 1, 0], dtype=int32)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0uirgCUTVy6"
      },
      "source": [
        "### train-test split for batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azpgwT5BTWHg"
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all)/BATCH_SIZE) # get number of batches (ceil(3.2)==4)\r\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10 # we just want the 10% of the data for testing purposes\r\n",
        "all_batched.shuffle(NB_BATCHES) # shuffle it (we give it NB_BATCHES, and not NB_BATCHES*batch_size because we want to keep the integrity of each batch)\r\n",
        "test_data_set = all_batched.take(NB_BATCHES_TEST) # take the fist 10% as test\r\n",
        "train_data_set = all_batched.skip(NB_BATCHES_TEST) # skip the 10% to get the other 90% as train"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM5hWUTnEJ6H"
      },
      "source": [
        "# Step 3: Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X7Wj901fsrW"
      },
      "source": [
        "### lets see what the bert_layer gives as it's output (VERY IMPORTANT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWH3nQr-fIS_",
        "outputId": "ce54b98d-cc22-4c8f-ea21-ddfb4ecc9175"
      },
      "source": [
        "sample_sent = \"Roses are red.\"\r\n",
        "my_sent = [\"[CLS]\"] + tokenizer.tokenize(sample_sent) +[\"[SEP]\"]\r\n",
        "# we expand dim to simulate a batch....\r\n",
        "bert_layer([tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0),\r\n",
        "            tf.expand_dims(tf.cast(get_mask(my_sent), tf.int32), 0),\r\n",
        "            tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0),\r\n",
        "            ])\r\n",
        "# the CLS is: (1, 768) which 1 is batch_size. and 768 is the embedding dimension of bert. (*** this specifies the embedding for the whole sequence which we use in our classifier ***)\r\n",
        "# the words are: (1, S, 768) which 1 is batch_size. S is the sentence number of tokens. and 768 is the embedding dimension of bert."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
              " array([[-9.27935600e-01, -4.10335928e-01, -9.65755284e-01,\n",
              "          9.07318294e-01,  8.12914550e-01, -1.74174860e-01,\n",
              "          9.11234915e-01,  3.41952562e-01, -8.74521971e-01,\n",
              "         -9.99989390e-01, -7.78410733e-01,  9.69385445e-01,\n",
              "          9.86160517e-01,  6.36964917e-01,  9.48631406e-01,\n",
              "         -7.51193941e-01, -4.58340079e-01, -7.08104968e-01,\n",
              "          4.62099075e-01, -6.57927752e-01,  7.60415316e-01,\n",
              "          9.99994755e-01, -3.96862566e-01,  3.44166636e-01,\n",
              "          6.16489470e-01,  9.94400144e-01, -7.76634395e-01,\n",
              "          9.38316643e-01,  9.59452212e-01,  7.32879639e-01,\n",
              "         -6.93437755e-01,  2.93080807e-01, -9.93785501e-01,\n",
              "         -1.64552301e-01, -9.67019856e-01, -9.95549619e-01,\n",
              "          5.32936037e-01, -6.88061118e-01,  1.34707419e-02,\n",
              "          2.98189707e-02, -9.18356717e-01,  4.20526713e-01,\n",
              "          9.99988854e-01,  2.52677470e-01,  6.06236696e-01,\n",
              "         -3.50750387e-01, -9.99999762e-01,  4.97585893e-01,\n",
              "         -8.95187497e-01,  9.62561190e-01,  9.43731129e-01,\n",
              "          9.03286159e-01,  1.54699937e-01,  5.86144209e-01,\n",
              "          5.80860853e-01, -4.05053616e-01, -2.76634973e-02,\n",
              "          2.98046619e-01, -2.83076078e-01, -6.47424579e-01,\n",
              "         -6.51524067e-01,  5.43848217e-01, -9.56302404e-01,\n",
              "         -9.22750473e-01,  9.61463094e-01,  8.27476263e-01,\n",
              "         -3.50113600e-01, -4.06406432e-01, -8.74324664e-02,\n",
              "         -9.98739600e-02,  8.96688581e-01,  3.00932348e-01,\n",
              "         -1.51130766e-01, -8.52713823e-01,  8.09593379e-01,\n",
              "          4.00989741e-01, -6.61606550e-01,  9.99999762e-01,\n",
              "         -6.16246581e-01, -9.86407101e-01,  8.90943646e-01,\n",
              "          8.11158597e-01,  5.81395268e-01, -6.33874834e-01,\n",
              "          3.78200322e-01, -9.99999762e-01,  6.76352024e-01,\n",
              "         -2.30613098e-01, -9.92552519e-01,  3.85461807e-01,\n",
              "          6.57651365e-01, -2.90106207e-01,  4.46835548e-01,\n",
              "          6.28524601e-01, -5.58410704e-01, -6.65295780e-01,\n",
              "         -4.72273141e-01, -9.28039730e-01, -3.54473233e-01,\n",
              "         -6.19736671e-01,  1.24535732e-01, -3.48906338e-01,\n",
              "         -4.23185050e-01, -4.20835376e-01,  4.56588894e-01,\n",
              "         -6.14471495e-01, -5.15244126e-01,  5.01911223e-01,\n",
              "          4.29148644e-01,  7.59822369e-01,  4.37517256e-01,\n",
              "         -4.33598936e-01,  6.30962491e-01, -9.59743202e-01,\n",
              "          7.73877621e-01, -3.95738512e-01, -9.87354517e-01,\n",
              "         -6.73180699e-01, -9.92996335e-01,  7.77800798e-01,\n",
              "         -5.05858123e-01, -3.19991827e-01,  9.69388723e-01,\n",
              "         -3.51623029e-01,  3.79092544e-01, -2.21650392e-01,\n",
              "         -9.51505959e-01, -9.99999762e-01, -8.80427063e-01,\n",
              "         -8.34713757e-01, -2.77322263e-01, -4.70462054e-01,\n",
              "         -9.83712018e-01, -9.56730247e-01,  6.61121249e-01,\n",
              "          9.56025898e-01,  1.62189633e-01,  9.99961436e-01,\n",
              "         -5.11206865e-01,  9.59530592e-01, -5.58611453e-01,\n",
              "         -8.00221741e-01,  8.48544598e-01, -5.58320880e-01,\n",
              "          8.33739221e-01,  2.63151199e-01, -7.33847618e-01,\n",
              "          3.16190183e-01, -4.83307540e-01,  6.87450707e-01,\n",
              "         -7.94890821e-01, -3.81298512e-01, -8.71706903e-01,\n",
              "         -9.49488223e-01, -3.62460077e-01,  9.51175690e-01,\n",
              "         -7.62521684e-01, -9.61278260e-01, -1.53296500e-01,\n",
              "         -4.02463794e-01, -5.69816470e-01,  8.52476060e-01,\n",
              "          7.99818456e-01,  5.33586979e-01, -6.96547568e-01,\n",
              "          4.84273165e-01,  2.24443376e-01,  7.31195688e-01,\n",
              "         -8.18208277e-01, -3.58150542e-01,  5.32028973e-01,\n",
              "         -4.41676706e-01, -9.25721467e-01, -9.87607241e-01,\n",
              "         -5.07008731e-01,  5.31485975e-01,  9.93827164e-01,\n",
              "          7.66175568e-01,  4.12393868e-01,  8.83270621e-01,\n",
              "         -3.85667056e-01,  8.81850481e-01, -9.67345178e-01,\n",
              "          9.86407518e-01, -3.13536435e-01,  3.57364267e-01,\n",
              "         -6.57592416e-01,  2.70098746e-01, -8.59113097e-01,\n",
              "          2.32006937e-01,  8.62853467e-01, -9.03662741e-01,\n",
              "         -7.94610322e-01, -2.82699823e-01, -4.76583928e-01,\n",
              "         -5.01097977e-01, -8.80423069e-01,  5.45586765e-01,\n",
              "         -4.41978127e-01, -5.77729285e-01, -1.25810742e-01,\n",
              "          9.06032205e-01,  9.80562210e-01,  8.44253719e-01,\n",
              "          5.20121098e-01,  7.80579507e-01, -9.23414409e-01,\n",
              "         -5.87244749e-01,  2.34754771e-01,  2.97747076e-01,\n",
              "          3.54464412e-01,  9.96218681e-01, -8.01238298e-01,\n",
              "         -2.79998809e-01, -9.39948857e-01, -9.83751893e-01,\n",
              "          3.82998027e-02, -9.28821921e-01, -2.94138581e-01,\n",
              "         -7.00601518e-01,  7.67864525e-01, -3.51924598e-01,\n",
              "          6.57685220e-01,  5.54107964e-01, -9.90459442e-01,\n",
              "         -7.80434608e-01,  5.50273359e-01, -5.03933489e-01,\n",
              "          5.56853116e-01, -3.53224695e-01,  7.86350727e-01,\n",
              "          9.69607294e-01, -6.54102504e-01,  7.34233677e-01,\n",
              "          8.81996930e-01, -9.14772868e-01, -7.82535076e-01,\n",
              "          8.52741778e-01, -4.38668370e-01,  8.24172378e-01,\n",
              "         -7.77355015e-01,  9.91627038e-01,  9.48048770e-01,\n",
              "          7.74401844e-01, -9.52811956e-01, -7.52401650e-01,\n",
              "         -8.65391195e-01, -8.10666203e-01, -1.91085979e-01,\n",
              "          6.22569285e-02,  9.39343095e-01,  6.60155594e-01,\n",
              "          5.10393798e-01,  3.03852588e-01, -7.58786500e-01,\n",
              "          9.97947276e-01, -8.39391232e-01, -9.73821700e-01,\n",
              "         -6.96809232e-01, -4.71227348e-01, -9.92139935e-01,\n",
              "          9.27337348e-01,  3.11118901e-01,  6.17150486e-01,\n",
              "         -5.93245983e-01, -7.30744779e-01, -9.74081218e-01,\n",
              "          9.14184391e-01,  2.35107690e-01,  9.90232944e-01,\n",
              "         -4.98076737e-01, -9.59739745e-01, -7.62371898e-01,\n",
              "         -9.30913508e-01, -4.32061180e-02, -2.13117123e-01,\n",
              "         -6.06086910e-01, -2.81604957e-02, -9.69718456e-01,\n",
              "          6.36244774e-01,  6.35316730e-01,  5.37906170e-01,\n",
              "         -8.91036749e-01,  9.99303102e-01,  9.99999762e-01,\n",
              "          9.73003745e-01,  9.01396990e-01,  8.87466848e-01,\n",
              "         -9.99958813e-01, -6.90022886e-01,  9.99997914e-01,\n",
              "         -9.93730664e-01, -9.99999762e-01, -9.37510908e-01,\n",
              "         -8.12253833e-01,  2.70661980e-01, -9.99999762e-01,\n",
              "         -2.87079543e-01, -1.50921032e-01, -9.31140363e-01,\n",
              "          8.18556130e-01,  9.78328943e-01,  9.94965553e-01,\n",
              "         -9.99999762e-01,  8.81453872e-01,  9.30843294e-01,\n",
              "         -7.06027269e-01,  9.76767242e-01, -6.08330846e-01,\n",
              "          9.75543380e-01,  5.93020678e-01,  5.54319680e-01,\n",
              "         -2.44308010e-01,  4.22844052e-01, -9.68066633e-01,\n",
              "         -9.14158702e-01, -7.75703967e-01, -7.79754758e-01,\n",
              "          9.98873353e-01,  2.67526537e-01, -7.70681679e-01,\n",
              "         -9.30970490e-01,  6.98258877e-01, -1.79436460e-01,\n",
              "          1.48646221e-01, -9.69404459e-01, -3.27200413e-01,\n",
              "          7.69223571e-01,  8.38437974e-01,  2.74363875e-01,\n",
              "          4.46674615e-01, -6.88234270e-01,  4.38525677e-01,\n",
              "         -6.96177632e-02,  2.83886045e-01,  6.96685255e-01,\n",
              "         -9.55272734e-01, -5.49684763e-01, -3.89562428e-01,\n",
              "          3.75223786e-01, -7.64763415e-01, -9.54123020e-01,\n",
              "          9.69721258e-01, -4.86067265e-01,  9.72205877e-01,\n",
              "          9.99999762e-01,  7.64814556e-01, -9.13303673e-01,\n",
              "          6.57083571e-01,  4.31852698e-01, -7.01080620e-01,\n",
              "          9.99999762e-01,  8.67337525e-01, -9.83669698e-01,\n",
              "         -5.84472299e-01,  7.79541433e-01, -6.77890480e-01,\n",
              "         -7.74524271e-01,  9.99660909e-01, -3.41093630e-01,\n",
              "         -8.14480603e-01, -6.48070693e-01,  9.86273110e-01,\n",
              "         -9.94089246e-01,  9.97643113e-01, -8.94537747e-01,\n",
              "         -9.79997277e-01,  9.60477889e-01,  9.49232101e-01,\n",
              "         -6.83828712e-01, -7.17899203e-01,  2.86707520e-01,\n",
              "         -7.60042310e-01,  4.78333294e-01, -9.51963425e-01,\n",
              "          8.08321536e-01,  5.27614892e-01, -1.67665973e-01,\n",
              "          9.16268229e-01, -8.87899280e-01, -5.93431294e-01,\n",
              "          3.90308142e-01, -7.76923776e-01, -3.84819657e-01,\n",
              "          9.59038377e-01,  6.78381920e-01, -4.08703625e-01,\n",
              "         -1.99678466e-02, -4.68429357e-01, -7.41144001e-01,\n",
              "         -9.73734379e-01,  6.23254955e-01,  9.99999762e-01,\n",
              "         -4.31857020e-01,  8.94349456e-01, -5.72570503e-01,\n",
              "         -1.89501941e-02,  7.24848583e-02,  6.05421722e-01,\n",
              "          5.64564645e-01, -5.04035652e-01, -8.33653450e-01,\n",
              "          9.20378923e-01, -9.70664918e-01, -9.92627382e-01,\n",
              "          8.63120019e-01,  2.32819021e-01, -3.05339187e-01,\n",
              "          9.99999166e-01,  6.51025295e-01,  3.69559437e-01,\n",
              "          5.16952574e-01,  9.89937544e-01, -5.10570705e-02,\n",
              "          5.19781828e-01,  9.13520157e-01,  9.89344358e-01,\n",
              "         -4.06515300e-01,  6.72227979e-01,  8.66246402e-01,\n",
              "         -9.63321149e-01, -3.93905997e-01, -7.32534587e-01,\n",
              "          6.66506663e-02, -9.50429082e-01,  5.36760017e-02,\n",
              "         -9.64523673e-01,  9.78591084e-01,  9.72525299e-01,\n",
              "          5.02413392e-01,  3.42613518e-01,  8.20068121e-01,\n",
              "          9.99999762e-01, -8.37068498e-01,  5.97411990e-01,\n",
              "         -4.17202890e-01,  8.81286561e-01, -9.99911070e-01,\n",
              "         -8.37778509e-01, -4.66962457e-01, -2.72497445e-01,\n",
              "         -9.03815150e-01, -4.58638400e-01,  3.91834110e-01,\n",
              "         -9.79059398e-01,  9.10196960e-01,  8.29556227e-01,\n",
              "         -9.92893636e-01, -9.93933380e-01, -5.58823168e-01,\n",
              "          7.86012650e-01,  2.98601389e-01, -9.94314432e-01,\n",
              "         -8.16725969e-01, -6.58431828e-01,  9.07822311e-01,\n",
              "         -4.84596938e-01, -9.59578872e-01, -5.24702251e-01,\n",
              "         -4.26524192e-01,  5.39447963e-01, -3.51430088e-01,\n",
              "          6.03988528e-01,  8.84237468e-01,  6.91959500e-01,\n",
              "         -7.73554802e-01, -3.49988192e-01, -1.82106808e-01,\n",
              "         -8.09593081e-01,  9.06841695e-01, -8.09706628e-01,\n",
              "         -9.76247907e-01, -2.70706296e-01,  9.99999762e-01,\n",
              "         -5.54332912e-01,  8.93760979e-01,  7.55230308e-01,\n",
              "          7.80316710e-01, -1.99226230e-01,  3.35151792e-01,\n",
              "          9.55944419e-01,  3.82270932e-01, -7.57198095e-01,\n",
              "         -9.39320326e-01, -6.35582626e-01, -6.07329667e-01,\n",
              "          7.00572670e-01,  7.23614514e-01,  7.29012787e-01,\n",
              "          8.65883946e-01,  7.64538229e-01,  2.08821282e-01,\n",
              "         -6.98534921e-02, -5.63597598e-04,  9.99799371e-01,\n",
              "         -4.44100678e-01, -1.80672124e-01, -4.89860415e-01,\n",
              "         -2.91432649e-01, -4.25409913e-01, -1.98750794e-01,\n",
              "          9.99999762e-01,  3.56602371e-01,  7.75662184e-01,\n",
              "         -9.93823946e-01, -9.28071439e-01, -9.31738615e-01,\n",
              "          9.99999762e-01,  8.50040615e-01, -7.60715961e-01,\n",
              "          7.18036890e-01,  7.75469720e-01, -1.75162226e-01,\n",
              "          8.09469998e-01, -3.36548448e-01, -3.02385926e-01,\n",
              "          4.57468361e-01,  3.08044344e-01,  9.70232129e-01,\n",
              "         -6.18905008e-01, -9.75721300e-01, -5.94949424e-01,\n",
              "          5.63391447e-01, -9.66651201e-01,  9.99981225e-01,\n",
              "         -6.10341012e-01, -3.60576004e-01, -4.96436387e-01,\n",
              "         -4.91438359e-01,  4.47818249e-01,  2.87395045e-02,\n",
              "         -9.83154833e-01, -3.47387552e-01,  3.09111297e-01,\n",
              "          9.66638982e-01,  3.75864476e-01, -6.41107082e-01,\n",
              "         -8.90265167e-01,  8.92269909e-01,  8.32000673e-01,\n",
              "         -9.59132433e-01, -9.57766414e-01,  9.71166372e-01,\n",
              "         -9.84971106e-01,  7.67819822e-01,  9.99999762e-01,\n",
              "          3.83999139e-01,  4.38053071e-01,  3.52292985e-01,\n",
              "         -4.46137518e-01,  4.46569890e-01, -6.90632761e-01,\n",
              "          6.74426198e-01, -9.59155858e-01, -4.53285873e-01,\n",
              "         -2.96153188e-01,  3.57684880e-01, -2.41154909e-01,\n",
              "         -5.88315129e-01,  7.63308406e-01,  3.13667834e-01,\n",
              "         -6.03100955e-01, -6.84795916e-01, -2.60148019e-01,\n",
              "          5.75160742e-01,  9.16844428e-01, -3.56800675e-01,\n",
              "         -2.31558591e-01,  1.15728445e-01, -1.77119672e-01,\n",
              "         -9.47563767e-01, -5.23142695e-01, -6.04618192e-01,\n",
              "         -9.99998629e-01,  5.41668594e-01, -9.99999762e-01,\n",
              "          6.60003424e-01,  3.39038610e-01, -2.57962316e-01,\n",
              "          8.98434281e-01,  3.58504117e-01,  7.80092597e-01,\n",
              "         -8.63456368e-01, -9.04244244e-01,  2.35172078e-01,\n",
              "          8.47542286e-01, -4.83705610e-01, -7.76437879e-01,\n",
              "         -7.77087152e-01,  4.51547921e-01, -1.20644890e-01,\n",
              "          3.45338553e-01, -7.58305609e-01,  7.38663971e-01,\n",
              "         -2.54878908e-01,  9.99999762e-01,  1.56727195e-01,\n",
              "         -6.47173643e-01, -9.80846524e-01,  3.21545452e-01,\n",
              "         -3.49480897e-01,  9.99999762e-01, -8.88086557e-01,\n",
              "         -9.70758796e-01,  4.17614907e-01, -6.59507155e-01,\n",
              "         -8.39062095e-01,  4.56446469e-01,  7.08395317e-02,\n",
              "         -8.59649241e-01, -9.68725860e-01,  9.56584096e-01,\n",
              "          8.95311415e-01, -6.79162920e-01,  7.91996717e-01,\n",
              "         -3.77205342e-01, -5.99683046e-01,  1.89220026e-01,\n",
              "          9.34770763e-01,  9.87944543e-01,  7.07965672e-01,\n",
              "          9.21088040e-01, -1.59542650e-01, -4.83468860e-01,\n",
              "          9.76640522e-01,  2.95252740e-01,  5.32054245e-01,\n",
              "          3.22658926e-01,  9.99999762e-01,  4.97992367e-01,\n",
              "         -9.31001008e-01, -3.24746013e-01, -9.82841671e-01,\n",
              "         -2.67997146e-01, -9.52166080e-01,  4.53917265e-01,\n",
              "          3.94373298e-01,  9.26190436e-01, -3.09753150e-01,\n",
              "          9.69366550e-01, -9.40264285e-01,  1.66798681e-01,\n",
              "         -8.32233727e-01, -7.04412758e-01,  5.49371183e-01,\n",
              "         -9.30373728e-01, -9.88702476e-01, -9.91572261e-01,\n",
              "          7.38683283e-01, -5.26327610e-01, -9.29534286e-02,\n",
              "          2.77956545e-01,  2.54385829e-01,  5.55893838e-01,\n",
              "          5.70780933e-01, -9.99999762e-01,  9.51999664e-01,\n",
              "          5.82981527e-01,  9.13394749e-01,  9.78624403e-01,\n",
              "          7.49033153e-01,  7.39972234e-01,  3.71187180e-01,\n",
              "         -9.89660680e-01, -9.84848619e-01, -5.31398356e-01,\n",
              "         -3.88979793e-01,  8.49414051e-01,  8.17017615e-01,\n",
              "          8.92697096e-01,  6.16892815e-01, -5.75284600e-01,\n",
              "         -2.86468834e-01, -7.60572135e-01, -7.78940201e-01,\n",
              "         -9.94441748e-01,  5.72189033e-01, -7.72195876e-01,\n",
              "         -9.57696497e-01,  9.67420816e-01, -2.17977047e-01,\n",
              "         -1.75553411e-01, -3.26558799e-01, -9.06778574e-01,\n",
              "          9.35597599e-01,  7.66240060e-01,  1.90596953e-01,\n",
              "          1.53929532e-01,  5.40217876e-01,  9.02483582e-01,\n",
              "          9.40389097e-01,  9.88884926e-01, -9.10144448e-01,\n",
              "          7.88360298e-01, -8.31383705e-01,  6.11196160e-01,\n",
              "          8.21669459e-01, -9.41967785e-01,  3.75133961e-01,\n",
              "          5.49407184e-01, -6.15319669e-01,  3.91502053e-01,\n",
              "         -3.66628110e-01, -9.74752426e-01,  8.88878524e-01,\n",
              "         -3.62838924e-01,  6.53206527e-01, -5.35070539e-01,\n",
              "         -2.21294500e-02, -4.40158635e-01, -3.87567401e-01,\n",
              "         -7.89355397e-01, -6.70903385e-01,  6.87370539e-01,\n",
              "          4.34680998e-01,  9.07511055e-01,  9.13954675e-01,\n",
              "         -1.07545368e-01, -8.54992092e-01, -3.22966218e-01,\n",
              "         -7.80993879e-01, -9.35075462e-01,  9.56621170e-01,\n",
              "         -2.46968299e-01, -1.84678793e-01,  7.18142748e-01,\n",
              "          1.66835070e-01,  9.54272747e-01,  5.20281076e-01,\n",
              "         -5.11346877e-01, -3.58431071e-01, -7.76735067e-01,\n",
              "          9.01379168e-01, -6.45810843e-01, -6.68203890e-01,\n",
              "         -6.79576516e-01,  8.30889761e-01,  4.56940204e-01,\n",
              "          9.99998212e-01, -8.61588180e-01, -9.52708840e-01,\n",
              "         -5.74055374e-01, -4.75624174e-01,  5.11586487e-01,\n",
              "         -7.02608049e-01, -9.99999762e-01,  5.05624473e-01,\n",
              "         -6.52047276e-01,  8.13731849e-01, -8.72139931e-01,\n",
              "          8.09320688e-01, -8.18222642e-01, -9.88196433e-01,\n",
              "         -4.00083601e-01,  3.38947028e-01,  7.67014921e-01,\n",
              "         -5.16353250e-01, -8.75941157e-01,  6.15952611e-01,\n",
              "         -7.52710283e-01,  9.89328623e-01,  8.95710826e-01,\n",
              "         -6.14250958e-01,  2.19648466e-01,  7.52754748e-01,\n",
              "         -8.20390761e-01, -8.05691600e-01,  9.38039601e-01]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 6, 768), dtype=float32, numpy=\n",
              " array([[[-0.07947534,  0.00580747, -0.31414014, ..., -0.45097226,\n",
              "           0.2933321 ,  0.23387566],\n",
              "         [ 0.39316174,  0.50336534,  0.2402173 , ..., -0.32635736,\n",
              "           0.3498618 ,  0.20673233],\n",
              "         [ 0.3578918 ,  0.10767145, -0.04988673, ..., -0.50822896,\n",
              "           0.25048932, -0.262688  ],\n",
              "         [-0.29892358, -0.2470882 ,  0.07151709, ..., -0.33810043,\n",
              "           0.12699765, -0.09681816],\n",
              "         [-0.3681547 , -0.71465284, -0.2103249 , ...,  0.35395175,\n",
              "           0.33438614, -0.62334824],\n",
              "         [ 0.88692445, -0.16996735, -0.2917387 , ...,  0.05816775,\n",
              "          -0.57760024, -0.32075334]]], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOI5-vrx9_rH"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywbYhlVQExPY"
      },
      "source": [
        "class DCNN_BERT_Embedding(tf.keras.Model):\r\n",
        "    def __init__(self,\r\n",
        "                 nb_filters=50,\r\n",
        "                 FFN_units=512,\r\n",
        "                 nb_classes=2,\r\n",
        "                 dropout_rate=0.1,\r\n",
        "                 training=False,\r\n",
        "                 name='DCNN_BERT_Embedding'):\r\n",
        "        super(DCNN_BERT_Embedding, self).__init__(name=name)\r\n",
        "        \r\n",
        "        # self.embedding = layers.Embedding(vocab_size, emb_dim)  # we dont use this, instead we use bert's embedding\r\n",
        "        self.bert_layer = hub.KerasLayer(\r\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\r\n",
        "            trainable=False)\r\n",
        "        \r\n",
        "        # bigram is conv on 2-grams (kernel_size=2)\r\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\r\n",
        "                                    kernel_size=2,\r\n",
        "                                    padding='valid',\r\n",
        "                                    activation='relu')\r\n",
        "\r\n",
        "        self.trigram =  layers.Conv1D(filters=nb_filters,\r\n",
        "                                      kernel_size=3,\r\n",
        "                                      padding='valid',\r\n",
        "                                      activation='relu')\r\n",
        "        \r\n",
        "        self.fourgram =  layers.Conv1D(filters=nb_filters,\r\n",
        "                                       kernel_size=4,\r\n",
        "                                       padding='valid',\r\n",
        "                                       activation='relu')\r\n",
        "        \r\n",
        "        self.pool = layers.GlobalMaxPool1D()     \r\n",
        "\r\n",
        "        self.dense_1 = layers.Dense(units=FFN_units,\r\n",
        "                                    activation='relu')\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "\r\n",
        "        # u could use softmax for binary too, but sigmoid with only 1 unit is\r\n",
        "        # used always so why not just use it here too...\r\n",
        "        if nb_classes == 2:\r\n",
        "            self.last_dense = layers.Dense(units=1, activation='sigmoid')\r\n",
        "        else:\r\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\r\n",
        "                                           activation='softmax')\r\n",
        "\r\n",
        "    ########################################################################################################### new:\r\n",
        "    def embed_with_bert(self, all_tokens): # this is a function like the regular embedding layer\r\n",
        "        # the '_' represents the whole sentence but we want the words\r\n",
        "        # all_tokens=(batch, 3(id_mask_segment), values(d_model))\r\n",
        "        _, embs = self.bert_layer([all_tokens[:, 0, :], # the inputs are just like the previous cell's example\r\n",
        "                                   all_tokens[:, 1, :],\r\n",
        "                                   all_tokens[:, 2, :]])\r\n",
        "        return embs\r\n",
        "\r\n",
        "    def call(self, inputs, training):\r\n",
        "        # training is boolean, when it's false we don't do dropout\r\n",
        "        x = self.embed_with_bert(inputs) # (batch_size, d_embeded(128))\r\n",
        "\r\n",
        "        # we get outputs for all 3 kernelsizes independentaly (we concat them later)\r\n",
        "        x_1 = self.bigram(x)\r\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters, 128-2+1)\r\n",
        "\r\n",
        "        x_2 = self.trigram(x)\r\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters, 128-3+1)\r\n",
        "\r\n",
        "        x_3 = self.fourgram(x)\r\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters, 128-4+1)\r\n",
        "\r\n",
        "        merged = layers.concatenate([x_1,x_2,x_3], axis=-1) # (batch_size, 3 * nb_filters, (128*3-2-3-4+3))\r\n",
        "\r\n",
        "        merged = self.dense_1(merged)\r\n",
        "        merged = self.dropout(merged, training=training)\r\n",
        "        \r\n",
        "        output = self.last_dense(merged)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xnXGfBdEJxn"
      },
      "source": [
        "# Step 4: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agNUC9xToAcR"
      },
      "source": [
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL_v0AkhExtY"
      },
      "source": [
        "NB_FILTERS = 100\r\n",
        "FFN_UNITS = 256\r\n",
        "NB_CLASSES = 2\r\n",
        "DROPOUT_RATE = 0.25\r\n",
        "NB_EPOCHS = 2"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWnNKVDboIpa"
      },
      "source": [
        "### Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7l6lZZBoKah"
      },
      "source": [
        "Dcnn = DCNN_BERT_Embedding(\r\n",
        "            nb_filters=NB_FILTERS,\r\n",
        "            FFN_units=FFN_UNITS,\r\n",
        "            nb_classes=NB_CLASSES,\r\n",
        "            dropout_rate=DROPOUT_RATE)\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whb4wPB8ovKx"
      },
      "source": [
        "if NB_CLASSES == 2:\r\n",
        "     Dcnn.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer='adam',\r\n",
        "                  metrics=['accuracy'])    \r\n",
        "else:\r\n",
        "     Dcnn.compile(loss='sparse_categorical_crossentropy',\r\n",
        "                  optimizer='adam',\r\n",
        "                  metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCFdXyabpAkw"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daZp6vLUo_mp"
      },
      "source": [
        "checkpoint_path = \"ckpt2\"\r\n",
        "\r\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\r\n",
        "\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\r\n",
        "\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "    print(\"Latest chekpoint has been restored!\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olKb-XUHpyri"
      },
      "source": [
        "### Custom call back"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCvigWyspsLp"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\r\n",
        "    # we basically tell tensorflow what it should print or do after each epoch ends\r\n",
        "    # this on_epoch_end actually exists in tf.keras.callbacks so we just call it\r\n",
        "    # that MyCustomCallback has inherited\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        ckpt_manager.save()\r\n",
        "        print(f\"Checkpoint saved!\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_BOLP0qgwZ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_SeS-eFqjNQ",
        "outputId": "6a4ce84d-49e7-4aaf-851f-2f4797a7fc4d"
      },
      "source": [
        "Dcnn.fit(train_data_set,\r\n",
        "         validation_data=test_data_set,\r\n",
        "         epochs=NB_EPOCHS,\r\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "44780/44780 [==============================] - 2494s 55ms/step - loss: 0.3798 - accuracy: 0.8333 - val_loss: 0.3541 - val_accuracy: 0.8483\n",
            "Checkpoint saved!\n",
            "Epoch 2/2\n",
            "44780/44780 [==============================] - 2457s 54ms/step - loss: 0.3468 - accuracy: 0.8514 - val_loss: 0.3454 - val_accuracy: 0.8535\n",
            "Checkpoint saved!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f529b69a2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}